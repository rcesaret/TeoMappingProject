{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis of TMP Databases\n",
    "\n",
    "**Project:** Digital Teotihuacan Mapping Project (TMP) - Phase 1\n",
    "\n",
    "**Objective:** This notebook synthesizes the results from the entire profiling pipeline to conduct a comparative analysis of the four legacy databases and the two wide-format benchmark databases. Its primary goal is to use quantitative data to compare these database architectures on three key axes: **Structural Complexity**, **Resource Usage**, and **Query Performance**. \n",
    "\n",
    "The findings from this notebook will directly inform the final recommendation for the Phase 2 unified database architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Path Definitions ---\n",
    "# Assumes the notebook is run from the 'notebooks' directory.\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "REPORTS_DIR = PROJECT_ROOT / \"outputs\" / \"reports\"\n",
    "\n",
    "# --- Styling and Display Options ---\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "def display_header(title):\n",
    "    display(Markdown(f'### {title}'))\n",
    "\n",
    "print(\"âœ… Setup complete.\")\n",
    "print(f\"Reports Directory: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Data Loading\n",
    "We load two key outputs from the `04_run_comparison.py` script:\n",
    "1. `comparison_matrix.csv`: High-level summary metrics.\n",
    "2. `report_performance_summary_detailed.csv`: The enriched, long-form performance data with calculated comparative metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_path = REPORTS_DIR / 'comparison_matrix.csv'\n",
    "perf_path = REPORTS_DIR / 'report_performance_summary_detailed.csv'\n",
    "\n",
    "if not matrix_path.exists() or not perf_path.exists():\n",
    "    raise FileNotFoundError(f\"Critical Error: Report files not found in {REPORTS_DIR}. Please run the 04_run_comparison.py script first.\")\n",
    "\n",
    "# Load the matrix and transpose it so databases are rows\n",
    "comparison_df = pd.read_csv(matrix_path, index_col=0).T.reset_index().rename(columns={'index': 'Database'})\n",
    "\n",
    "# Load the detailed performance data\n",
    "perf_summary_df = pd.read_csv(perf_path)\n",
    "\n",
    "print(\"Loaded Comparison Matrix:\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nLoaded Detailed Performance Summary Data:\")\n",
    "display(perf_summary_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. High-Level Comparison Matrix\n",
    "A styled view of the main comparison matrix. Color gradients highlight high/low values for each metric, providing an at-a-glance summary.\n",
    "- <span style='color: #440154;'>**Purple/Dark**</span>: Higher values\n",
    "- <span style='color: #fde725;'>**Yellow/Light**</span>: Lower values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Styled Comparison Matrix\")\n",
    "\n",
    "styled_df = comparison_df.style.background_gradient(cmap='viridis', axis=0)\\\n",
    "    .set_caption(\"Comparative Database Metrics\")\\\n",
    "    .format('{:.2f}', subset=pd.IndexSlice[:, ['Database Size (MB)', 'JDI (Join Dependency Index)', 'NF (Normalization Factor)']])\\\n",
    "    .format('{:,.0f}', subset=pd.IndexSlice[:, ['Table Count', 'View Count', 'Total Estimated Rows', 'Total Index Count', 'LIF (Logical Interop. Factor)']])\n",
    "\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Structural Complexity Analysis\n",
    "This section focuses on the metrics that quantify the relational complexity and degree of normalization of the legacy schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Schema Complexity Metrics (Legacy Databases)\")\n",
    "\n",
    "complexity_metrics = [\n",
    "    'Database', 'Table Count', \n",
    "    'JDI (Join Dependency Index)', \n",
    "    'LIF (Logical Interop. Factor)',\n",
    "    'NF (Normalization Factor)'\n",
    "]\n",
    "# Filter for legacy DBs only, as these metrics don't apply to the single-table benchmarks\n",
    "legacy_df = comparison_df[~comparison_df['Database'].str.contains('benchmark')]\n",
    "display(legacy_df[complexity_metrics])\n",
    "\n",
    "# --- Advanced Visualization: Complexity Radar Plot ---\n",
    "radar_metrics = ['Table Count', 'JDI (Join Dependency Index)', 'NF (Normalization Factor)']\n",
    "radar_df = legacy_df[['Database'] + radar_metrics].copy()\n",
    "\n",
    "# Normalize metrics to a 0-1 scale for fair comparison on the radar plot\n",
    "scaler = MinMaxScaler()\n",
    "radar_df[radar_metrics] = scaler.fit_transform(radar_df[radar_metrics])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for index, row in radar_df.iterrows():\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=row[radar_metrics].values,\n",
    "        theta=radar_metrics,\n",
    "        fill='toself',\n",
    "        name=row['Database']\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      range=[0, 1]\n",
    "    )),\n",
    "  showlegend=True,\n",
    "  title='Normalized Complexity Profile of Legacy Databases'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5. Query Performance Deep Dive\n",
    "This is the most critical comparison. It directly measures the analytical query performance of the legacy normalized schemas against the denormalized wide-format benchmark schemas using the pre-calculated metrics from the `04_run_comparison.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Schema Efficiency Factor by Query Category (Log Scale)\")\n",
    "\n",
    "if not perf_summary_df.empty:\n",
    "    fig = px.bar(perf_summary_df,\n",
    "                 x='database', \n",
    "                 y='schema_efficiency_factor', \n",
    "                 color='category', \n",
    "                 barmode='group',\n",
    "                 title='Schema Efficiency Factor (Lower is Better)',\n",
    "                 labels={'schema_efficiency_factor': 'Efficiency Factor (Log Scale)', 'database': 'Database'},\n",
    "                 category_orders={'category': ['baseline', 'join_performance', 'complex_filtering']})\n",
    "    \n",
    "    fig.update_yaxes(type=\"log\") # Use a log scale as differences can be huge\n",
    "    fig.add_hline(y=1.0, line_dash=\"dot\", annotation_text=\"Benchmark Baseline\", annotation_position=\"bottom right\")\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No performance data to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Performance Improvement vs. Best Benchmark\")\n",
    "\n",
    "if not perf_summary_df.empty:\n",
    "    # Filter out the baseline databases themselves for a cleaner plot\n",
    "    improvement_df = perf_summary_df[~perf_summary_df['database'].str.contains('benchmark')].copy()\n",
    "    \n",
    "    fig = px.bar(improvement_df.sort_values('performance_improvement_factor'),\n",
    "                 x='query_id', \n",
    "                 y='performance_improvement_factor', \n",
    "                 color='database', \n",
    "                 facet_row='category',\n",
    "                 barmode='group',\n",
    "                 title='Performance Improvement of Benchmark Schemas vs. Legacy Schemas',\n",
    "                 labels={'performance_improvement_factor': '% Improvement vs. Benchmark', 'query_id': 'Query ID'})\n",
    "    \n",
    "    fig.update_layout(height=800)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Could not generate performance improvement plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 6. Qualitative Architectural Trade-offs\n",
    "The quantitative data above supports a qualitative assessment of the architectural trade-offs between the legacy design and the proposed wide-format design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                  | Legacy Normalized (e.g., DF9)      | Proposed Wide-Format (Benchmark)   | Justification Based on Data                                                                      |\n",
    "| :----------------------- | :--------------------------------- | :--------------------------------- | :----------------------------------------------------------------------------------------------- |\n",
    "| **Query Performance** | `Low`                              | `High`                             | The 'Schema Efficiency Factor' chart shows legacy databases are multiple times slower.             |\n",
    "| **Storage Cost** | `Low`                              | `High`                             | `comparison_matrix.csv` shows benchmark DBs are larger due to data duplication.                |\n",
    "| **Schema Complexity** | `High` (High JDI/NF, Many Tables)  | `Very Low` (1 Table)               | The complexity radar plot visually confirms the high complexity scores of the legacy schemas.    |\n",
    "| **Data Redundancy** | `Low` (Normalized)                 | `High` (Denormalized)              | This is the inherent trade-off of the wide-format design; we trade storage for speed.            |\n",
    "| **Ease of Use for BI/GIS** | `Low` (Requires complex joins)     | `High` (Single table source)       | A single flat table is trivial to connect to tools like QGIS, Tableau, or Power BI.              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Final Analyst Summary & Recommendation\n",
    "\n",
    "**Instructions:** Based on the comparative analysis, synthesize the findings and provide a formal recommendation for the Phase 2 unified database architecture. This summary will be a primary input for the final white paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overarching Conclusion:\n",
    "* *Start with a concise, definitive statement. Example: \"The comparative analysis demonstrates conclusively that the highly normalized structure of the legacy databases, particularly `tmp_df9`, is quantitatively inferior for the project's analytical objectives compared to a denormalized, wide-format architecture.\"*\n",
    "\n",
    "### Justification from Evidence:\n",
    "1.  **On Performance:**\n",
    "    * *Quantify the performance difference. Reference the 'Schema Efficiency Factor' chart directly. Example: \"As shown in the efficiency factor plot, the legacy schemas are between 5x and 50x slower for join-heavy queries than the wide-format benchmarks. This performance gap makes interactive analysis on the normalized schemas untenable.\"*\n",
    "2.  **On Complexity:**\n",
    "    * *Reference the complexity metrics and the radar plot. Example: \"The legacy schemas exhibit high JDI and NF scores, indicative of significant relational complexity that increases the cognitive load for analysts and the technical barrier for connecting to BI and GIS tools. The radar plot clearly visualizes `tmp_df9` as the most complex outlier.\"*\n",
    "3.  **On The Cost/Benefit Trade-off:**\n",
    "    * *Acknowledge the trade-offs identified in the qualitative table. Example: \"While the wide-format approach increases storage costs due to data redundancy, this trade-off is strategically acceptable. The cost of storage is minimal compared to the significant gains in query performance and the drastic reduction in development time and analytical friction for end-users.\"*\n",
    "\n",
    "### Formal Recommendation:\n",
    "* *State the final recommendation clearly and unambiguously.*\n",
    "* **Recommended Architecture:** \"It is the formal recommendation of this analysis that Phase 2 of the Digital TMP project proceeds with the development of a single, denormalized, wide-format primary analytical table. This table should be based on the schema of the `tmp_benchmark_wide_text_nulls` database, as it provides the best balance of performance and human-readability.\"*\n",
    "* **Next Steps:** \"The next step should be to finalize the schema of this wide-format table, including data type assignments and column naming conventions, and to proceed with the development of the full ETL pipeline in Phase 2 to migrate all legacy data into this new structure.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}