{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis of TMP Databases\n",
    "\n",
    "**Project:** Digital Teotihuacan Mapping Project (TMP) - Phase 1\n",
    "\n",
    "**Objective:** This notebook synthesizes the results from the entire profiling pipeline to conduct a comparative analysis of the four legacy databases and the two wide-format benchmark databases. Its primary goal is to use quantitative data to compare these database architectures on three key axes: **Structural Complexity**, **Resource Usage**, and **Query Performance**. \n",
    "\n",
    "The findings from this notebook will directly inform the final recommendation for the Phase 2 unified database architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Path Definitions ---\n",
    "# Assumes the notebook is run from the 'notebooks' directory.\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "REPORTS_DIR = PROJECT_ROOT / \"outputs\" / \"reports\"\n",
    "METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n",
    "\n",
    "# --- Styling and Display Options ---\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "def display_header(title):\n",
    "    display(Markdown(f'### {title}'))\n",
    "\n",
    "print(\"âœ… Setup complete.\")\n",
    "print(f\"Reports Directory: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Data Loading\n",
    "Load the master `comparison_matrix.csv` and the detailed performance benchmark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_path = REPORTS_DIR / 'comparison_matrix.csv'\n",
    "\n",
    "if not matrix_path.exists():\n",
    "    raise FileNotFoundError(f\"Critical Error: comparison_matrix.csv not found at {matrix_path}. Please run the 04_run_comparison.py script first.\")\n",
    "\n",
    "# Load the matrix and transpose it so databases are rows for easier analysis\n",
    "comparison_df = pd.read_csv(matrix_path, index_col=0).T.reset_index().rename(columns={'index': 'Database'})\n",
    "\n",
    "# Load all individual performance files to create a detailed performance dataframe\n",
    "perf_files = list(METRICS_DIR.glob('*_performance_benchmarks.csv'))\n",
    "if perf_files:\n",
    "    perf_df_list = [pd.read_csv(f).assign(Database=f.name.split('_')[0]) for f in perf_files]\n",
    "    full_perf_df = pd.concat(perf_df_list, ignore_index=True)\n",
    "else:\n",
    "    full_perf_df = pd.DataFrame()\n",
    "\n",
    "print(\"Loaded Comparison Matrix:\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nLoaded Full Performance Data:\")\n",
    "if not full_perf_df.empty:\n",
    "    display(full_perf_df.head())\n",
    "else:\n",
    "    print(\"No performance data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. High-Level Comparison Matrix\n",
    "A styled view of the main comparison matrix. Color gradients highlight high/low values for each metric, providing an at-a-glance summary.\n",
    "- <span style='color: #440154;'>**Purple/Dark**</span>: Higher values\n",
    "- <span style='color: #fde725;'>**Yellow/Light**</span>: Lower values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Styled Comparison Matrix\")\n",
    "\n",
    "styled_df = comparison_df.style.background_gradient(cmap='viridis', axis=0)\\\n",
    "    .set_caption(\"Comparative Database Metrics\")\\\n",
    "    .format('{:.2f}', subset=pd.IndexSlice[:, ['Database Size (MB)', 'JDI (Join Dependency Index)', 'NF (Normalization Factor)']])\\\n",
    "    .format('{:,.0f}', subset=pd.IndexSlice[:, ['Table Count', 'View Count', 'Total Estimated Rows', 'Total Index Count', 'LIF (Logical Interop. Factor)']])\n",
    "\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Structural Complexity Analysis\n",
    "This section focuses on the metrics that quantify the relational complexity and degree of normalization of the legacy schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Schema Complexity Metrics (Legacy Databases)\")\n",
    "\n",
    "complexity_metrics = [\n",
    "    'Database', 'Table Count', \n",
    "    'JDI (Join Dependency Index)', \n",
    "    'LIF (Logical Interop. Factor)',\n",
    "    'NF (Normalization Factor)'\n",
    "]\n",
    "# Filter for legacy DBs only, as these metrics don't apply to the single-table benchmarks\n",
    "legacy_df = comparison_df[~comparison_df['Database'].str.contains('benchmark')]\n",
    "display(legacy_df[complexity_metrics])\n",
    "\n",
    "# --- Advanced Visualization: Complexity Radar Plot ---\n",
    "radar_metrics = ['Table Count', 'JDI (Join Dependency Index)', 'NF (Normalization Factor)']\n",
    "radar_df = legacy_df[['Database'] + radar_metrics].copy()\n",
    "\n",
    "# Normalize metrics to a 0-1 scale for fair comparison on the radar plot\n",
    "scaler = MinMaxScaler()\n",
    "radar_df[radar_metrics] = scaler.fit_transform(radar_df[radar_metrics])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for index, row in radar_df.iterrows():\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=row[radar_metrics].values,\n",
    "        theta=radar_metrics,\n",
    "        fill='toself',\n",
    "        name=row['Database']\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      range=[0, 1]\n",
    "    )),\n",
    "  showlegend=True,\n",
    "  title='Normalized Complexity Profile of Legacy Databases'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5. Query Performance Deep Dive\n",
    "This is the most critical comparison. It directly measures the analytical query performance of the legacy normalized schemas against the denormalized wide-format benchmark schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Query Performance Comparison (Latency in ms)\")\n",
    "\n",
    "if not full_perf_df.empty:\n",
    "    fig = px.bar(full_perf_df.sort_values('latency_ms', ascending=False),\n",
    "                 x='query_name', \n",
    "                 y='latency_ms', \n",
    "                 color='Database', \n",
    "                 barmode='group',\n",
    "                 title='Canonical Query Performance: Latency Comparison',\n",
    "                 labels={'latency_ms': 'Latency (ms) - Lower is Better', 'query_name': 'Canonical Query'})\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No performance data to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Performance Improvement vs. TMP_DF9 Baseline\")\n",
    "\n",
    "if not full_perf_df.empty and 'tmp_df9' in full_perf_df['Database'].unique():\n",
    "    baseline_perf = full_perf_df[full_perf_df['Database'] == 'tmp_df9'][['query_name', 'latency_ms']].rename(columns={'latency_ms': 'baseline_latency'})\n",
    "    \n",
    "    perf_comparison = pd.merge(full_perf_df, baseline_perf, on='query_name')\n",
    "    \n",
    "    # Calculate the improvement factor. Avoid division by zero.\n",
    "    perf_comparison['improvement_factor'] = perf_comparison['baseline_latency'] / perf_comparison['latency_ms'].replace(0, np.nan)\n",
    "    \n",
    "    # Filter out the baseline itself for a cleaner plot\n",
    "    plot_df = perf_comparison[perf_comparison['Database'] != 'tmp_df9']\n",
    "    \n",
    "    fig = px.bar(plot_df.sort_values('improvement_factor'),\n",
    "                 x='query_name', \n",
    "                 y='improvement_factor', \n",
    "                 color='Database', \n",
    "                 barmode='group',\n",
    "                 title='Performance Improvement Factor (vs. tmp_df9)',\n",
    "                 labels={'improvement_factor': 'Improvement Factor (e.g., 2.0x = Twice as Fast)', 'query_name': 'Canonical Query'})\n",
    "    \n",
    "    # Add a line at y=1 to indicate the baseline\n",
    "    fig.add_hline(y=1.0, line_dash=\"dot\", annotation_text=\"tmp_df9 Baseline\", annotation_position=\"bottom right\")\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Could not generate performance improvement plot. Baseline 'tmp_df9' data is missing or no performance data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 6. Qualitative Architectural Trade-offs\n",
    "The quantitative data above supports a qualitative assessment of the architectural trade-offs between the legacy design and the proposed wide-format design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                  | Legacy Normalized (e.g., DF9)      | Proposed Wide-Format (Benchmark)   | Justification Based on Data                                                                      |\n",
    "| :----------------------- | :--------------------------------- | :--------------------------------- | :----------------------------------------------------------------------------------------------- |\n",
    "| **Query Performance** | `Low`                              | `High`                             | The performance benchmark charts show a significant (often >10x) reduction in latency.             |\n",
    "| **Storage Cost** | `Low`                              | `High`                             | `comparison_matrix.csv` shows benchmark DBs are larger due to data duplication.                |\n",
    "| **Schema Complexity** | `High` (High JDI/NF, Many Tables)  | `Very Low` (1 Table)               | The complexity radar plot visually confirms the high complexity scores of the legacy schemas.    |\n",
    "| **Data Redundancy** | `Low` (Normalized)                 | `High` (Denormalized)              | This is the inherent trade-off of the wide-format design; we trade storage for speed.            |\n",
    "| **Ease of Use for BI/GIS** | `Low` (Requires complex joins)     | `High` (Single table source)       | A single flat table is trivial to connect to tools like QGIS, Tableau, or Power BI.              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Final Analyst Summary & Recommendation\n",
    "\n",
    "**Instructions:** Based on the comparative analysis, synthesize the findings and provide a formal recommendation for the Phase 2 unified database architecture. This summary will be a primary input for the final white paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overarching Conclusion:\n",
    "* *Start with a concise, definitive statement. Example: \"The comparative analysis demonstrates conclusively that the highly normalized structure of the legacy databases, particularly `tmp_df9`, is quantitatively inferior for the project's analytical objectives compared to a denormalized, wide-format architecture.\"*\n",
    "\n",
    "### Justification from Evidence:\n",
    "1.  **On Performance:**\n",
    "    * *Quantify the performance difference. Reference the 'Performance Improvement Factor' chart directly. Example: \"As shown in Figure [X], the wide-format benchmark databases performed, on average, [Y] times faster on representative analytical queries than the `tmp_df9` schema. For join-intensive queries, this improvement exceeded [Z]x.\"*\n",
    "2.  **On Complexity:**\n",
    "    * *Reference the complexity metrics and the radar plot. Example: \"The legacy schemas exhibit high JDI and NF scores, indicative of significant relational complexity that increases the cognitive load for analysts and the technical barrier for connecting to BI and GIS tools. The radar plot (Figure [X]) clearly visualizes `tmp_df9` as the most complex outlier.\"*\n",
    "3.  **On The Cost/Benefit Trade-off:**\n",
    "    * *Acknowledge the trade-offs identified in the qualitative table. Example: \"While the wide-format approach increases storage costs due to data redundancy, this trade-off is strategically acceptable. The cost of storage is minimal compared to the significant gains in query performance and the drastic reduction in development time and analytical friction for end-users.\"*\n",
    "\n",
    "### Formal Recommendation:\n",
    "* *State the final recommendation clearly and unambiguously.*\n",
    "* **Recommended Architecture:** \"It is the formal recommendation of this analysis that Phase 2 of the Digital TMP project proceeds with the development of a single, denormalized, wide-format primary analytical table. This table should be based on the schema of the `tmp_benchmark_wide_text_nulls` database, as it provides the best balance of performance and human-readability.\"*\n",
    "* **Next Steps:** \"The next step should be to finalize the schema of this wide-format table, including data type assignments and column naming conventions, and to proceed with the development of the full ETL pipeline in Phase 2 to migrate all legacy data into this new structure.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}