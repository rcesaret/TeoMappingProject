{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Database Analysis Report\n",
    "\n",
    "**Project:** Digital Teotihuacan Mapping Project (TMP) - Phase 1\n",
    "\n",
    "**Objective:** This notebook provides a comprehensive analysis of a single legacy or benchmark database. It loads the raw metric files generated by the `02_run_profiling_pipeline.py` script and produces a series of tables and visualizations to assess the database's structure, health, data content, and performance.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Set the `DATABASE_NAME`** in the configuration cell below.\n",
    "2.  **Execute all cells** from top to bottom (`Kernel > Restart & Run All`).\n",
    "3.  **Review the outputs** and complete the **Analyst Summary** section at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, SVG\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- CONFIGURATION ---------------------------------------------------\n",
    "# SET THIS VARIABLE to the name of the database you want to analyze.\n",
    "# e.g., 'tmp_df8', 'tmp_df9', 'tmp_benchmark_wide_numeric', etc.\n",
    "DATABASE_NAME = \"tmp_df9\" # <--- CHANGE THIS\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# --- Path Definitions ---\n",
    "# Assumes the notebook is run from the 'notebooks' directory.\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n",
    "ERDS_DIR = PROJECT_ROOT / \"outputs\" / \"erds\"\n",
    "\n",
    "# --- Styling and Display Options ---\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "def display_header(title):\n",
    "    display(Markdown(f'### {title}'))\n",
    "\n",
    "def load_metric_file(metric_name, file_type='csv'):\n",
    "    \"\"\"Helper function to safely load a metric file.\"\"\"\n",
    "    file_path = METRICS_DIR / f\"{DATABASE_NAME}_{metric_name}.{file_type}\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"⚠️ WARNING: Metric file not found: {file_path.name}\")\n",
    "        return None\n",
    "    if file_type == 'csv':\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_type == 'json':\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "print(f\"✅ Setup complete. Analyzing database: '{DATABASE_NAME}'\")\n",
    "print(f\"Metrics Directory: {METRICS_DIR}\")\n",
    "print(f\"ERD Directory: {ERDS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Data Loading\n",
    "Load all available metric files for the selected database. Warnings will be printed for any missing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all metric files into variables\n",
    "basic_metrics = load_metric_file('basic_metrics', 'json')\n",
    "schema_counts = load_metric_file('schema_counts', 'json')\n",
    "interop_metrics = load_metric_file('interop_metrics', 'json')\n",
    "\n",
    "table_metrics_df = load_metric_file('table_metrics')\n",
    "column_structure_df = load_metric_file('column_structure')\n",
    "column_profiles_df = load_metric_file('column_profiles')\n",
    "performance_df = load_metric_file('performance_benchmarks')\n",
    "\n",
    "print(\"✅ Data loading complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. High-Level Overview & Schema Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(f\"Key Metrics for: {DATABASE_NAME}\")\n",
    "\n",
    "summary_data = {}\n",
    "if basic_metrics: summary_data.update(basic_metrics)\n",
    "if schema_counts: summary_data.update(schema_counts)\n",
    "if interop_metrics: summary_data.update(interop_metrics)\n",
    "if table_metrics_df is not None:\n",
    "    summary_data['total_estimated_rows'] = int(table_metrics_df['row_estimate'].sum())\n",
    "\n",
    "if summary_data:\n",
    "    summary_series = pd.Series(summary_data).rename('Value').to_frame()\n",
    "    display(summary_series)\n",
    "else:\n",
    "    print(\"No summary metrics available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity-Relationship Diagram (ERD)\n",
    "The following diagram shows the full relational structure of the database schema. For complex schemas like `tmp_df9`, this may be very large and focused ERDs should be consulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(f\"Full ERD for: {DATABASE_NAME}\")\n",
    "\n",
    "try:\n",
    "    # Find the most recent ERD file for the database\n",
    "    erd_files = sorted(ERDS_DIR.glob(f\"{DATABASE_NAME}_full_ERD_*.svg\"), reverse=True)\n",
    "    if erd_files:\n",
    "        display(SVG(erd_files[0]))\n",
    "    else:\n",
    "        print(f\"❌ ERROR: Full ERD SVG file not found for '{DATABASE_NAME}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while displaying the ERD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Table-Level Analysis\n",
    "Analysis of the tables within the schema, focusing on size, row count, and health (bloat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Table Metrics Summary\")\n",
    "\n",
    "if table_metrics_df is not None and not table_metrics_df.empty:\n",
    "    display(table_metrics_df.sort_values(by='row_estimate', ascending=False).style.background_gradient(cmap='viridis', subset=['row_estimate', 'bloat_percent']))\n",
    "else:\n",
    "    print(\"No table metrics data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Largest Tables by Total Size and Bloat\")\n",
    "\n",
    "if table_metrics_df is not None and not table_metrics_df.empty:\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Top 10 Tables by Total Size\", \"Top 10 Tables by Bloat Size\")\n",
    "    )\n",
    "    \n",
    "    # Convert pretty size string to bytes for sorting\n",
    "    def size_to_bytes(s):\n",
    "        if not isinstance(s, str): return 0\n",
    "        num, unit = s.split()\n",
    "        num = float(num)\n",
    "        if 'KB' in unit: return num * 1024\n",
    "        if 'MB' in unit: return num * 1024**2\n",
    "        if 'GB' in unit: return num * 1024**3\n",
    "        return num\n",
    "    \n",
    "    df_copy = table_metrics_df.copy()\n",
    "    df_copy['total_bytes'] = df_copy['total_size'].apply(size_to_bytes)\n",
    "    df_copy['bloat_bytes_val'] = df_copy['bloat_bytes']\n",
    "\n",
    "    top_10_size = df_copy.nlargest(10, 'total_bytes')\n",
    "    top_10_bloat = df_copy.nlargest(10, 'bloat_bytes_val')\n",
    "\n",
    "    fig.add_trace(go.Bar(y=top_10_size['table_name'], x=top_10_size['total_bytes'], orientation='h', name='Total Size'), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(y=top_10_bloat['table_name'], x=top_10_bloat['bloat_bytes_val'], orientation='h', name='Bloat Size'), row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(title_text=f'Table Size Analysis for {DATABASE_NAME}', height=500, showlegend=False)\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.update_xaxes(title_text=\"Size (Bytes)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Bloat (Bytes)\", row=1, col=2)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No table metrics data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5. Column-Level Analysis\n",
    "A deep dive into the columns, focusing on data types, data completeness (NULL values), and complexity (cardinality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Data Type Distribution\")\n",
    "\n",
    "if column_structure_df is not None:\n",
    "    type_counts = column_structure_df['data_type'].value_counts().reset_index()\n",
    "    type_counts.columns = ['data_type', 'count']\n",
    "    \n",
    "    fig = px.bar(type_counts, x='data_type', y='count', title=f'Column Data Type Frequencies in {DATABASE_NAME}',\n",
    "                 labels={'count': 'Number of Columns', 'data_type': 'Data Type'})\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No column structure data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Completeness: NULL Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Top 20 Columns by Percentage of NULL Values\")\n",
    "\n",
    "if column_profiles_df is not None and not column_profiles_df.empty:\n",
    "    # Ensure we only show columns with NULLs\n",
    "    null_df = column_profiles_df[column_profiles_df['null_percent'] > 0].copy()\n",
    "    \n",
    "    if not null_df.empty:\n",
    "        # Create a full column identifier for clarity\n",
    "        null_df['full_column_name'] = null_df['tablename'] + '.' + null_df['column_name']\n",
    "        \n",
    "        top_20_nulls = null_df.nlargest(20, 'null_percent')\n",
    "        \n",
    "        fig = px.bar(top_20_nulls, \n",
    "                     y='full_column_name', \n",
    "                     x='null_percent', \n",
    "                     orientation='h',\n",
    "                     title=f'Top 20 Columns by NULL Percentage in {DATABASE_NAME}',\n",
    "                     labels={'null_percent': 'Percentage of Rows that are NULL (%)', 'full_column_name': 'Column'})\n",
    "        fig.update_layout(height=600)\n",
    "        fig.update_yaxes(autorange=\"reversed\")\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"✅ Excellent! No columns with NULL values were found.\")\n",
    "else:\n",
    "    print(\"No column profile data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Complexity: Cardinality Analysis\n",
    "Cardinality refers to the number of unique values in a column. \n",
    "\n",
    "- **Low Cardinality** columns (e.g., < 20 unique values) are often categorical codes or flags.\n",
    "- **High Cardinality** columns often represent unique identifiers or free-text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Column Cardinality Distribution\")\n",
    "\n",
    "if column_profiles_df is not None and not column_profiles_df.empty:\n",
    "    # Create a full column identifier\n",
    "    df = column_profiles_df.copy()\n",
    "    df['full_column_name'] = df['tablename'] + '.' + df['column_name']\n",
    "\n",
    "    # Display tables of highest and lowest cardinality columns\n",
    "    display(Markdown(\"**Columns with Highest Cardinality (Most Unique)**\"))\n",
    "    display(df.nlargest(10, 'distinct_values_estimate')[['full_column_name', 'distinct_values_estimate']])\n",
    "\n",
    "    display(Markdown(\"**Columns with Lowest Cardinality (Least Unique)**\"))\n",
    "    display(df[df['distinct_values_estimate'] > 1].nsmallest(10, 'distinct_values_estimate')[['full_column_name', 'distinct_values_estimate']])\n",
    "    \n",
    "    # Create a histogram of cardinalities to see the distribution\n",
    "    fig = px.histogram(df, x='distinct_values_estimate', log_y=True, \n",
    "                       title=f'Distribution of Column Cardinalities in {DATABASE_NAME}',\n",
    "                       labels={'distinct_values_estimate': 'Number of Distinct Values (Cardinality)'})\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No column profile data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 6. Performance Benchmark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_header(\"Canonical Query Performance Results\")\n",
    "\n",
    "if performance_df is not None and not performance_df.empty:\n",
    "    display(performance_df[['query_name', 'latency_ms', 'status']])\n",
    "    \n",
    "    # Plot the results for successful queries\n",
    "    success_df = performance_df[performance_df['status'] == 'Success']\n",
    "    if not success_df.empty:\n",
    "        fig = px.bar(success_df, x='query_name', y='latency_ms',\n",
    "                     title=f'Query Latency for {DATABASE_NAME}',\n",
    "                     labels={'latency_ms': 'Latency (ms)', 'query_name': 'Canonical Query'})\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"No performance benchmark data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Analyst Summary & Interpretation\n",
    "\n",
    "**Instructions:** Based on the data and visualizations above, provide a concise summary of your findings for the **`{DATABASE_NAME}`** database. Address the following points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "* *Your summary of the most striking features of this database. What is its primary characteristic (e.g., highly normalized, a few very large tables, excellent data completeness)?*\n",
    "\n",
    "### Data Quality & Health Concerns:\n",
    "* *Comment on the prevalence of NULL values. Are they concentrated in specific tables or columns? What might this imply about data collection or relevance?*\n",
    "* *Discuss the database bloat. Are there specific tables that are heavily bloated and may require maintenance (e.g., `VACUUM FULL`)?*\n",
    "\n",
    "### Structural Complexity:\n",
    "* *Analyze the relational structure based on the ERD and interoperability metrics (if applicable). Is the schema simple or complex? How does this relate to its JDI/NF score?*\n",
    "* *Discuss the data type usage. Is it appropriate for the data being stored?*\n",
    "\n",
    "### Performance Profile:\n",
    "* *Summarize the performance benchmark results. Were queries generally fast or slow? Were there any specific queries that were outliers?*\n",
    "\n",
    "### Implications for Phase 2 Redesign:\n",
    "* *Based on this analysis, what are the key strengths and weaknesses of this database's design?*\n",
    "* *What specific aspects of this schema should be preserved, changed, or discarded in the final unified database? (e.g., \"The high number of tables needs to be consolidated,\" or \"The use of boolean flags is effective and should be maintained.\")*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}