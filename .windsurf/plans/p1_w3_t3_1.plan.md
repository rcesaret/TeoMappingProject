---
plan_template_version: 1.0.0
last_updated: 2025-06-13
tags: [testing, python, pytest]
task_id: P1.W3.T3.1
description: Implement and pass pytest suite for `04_run_comparison.py` aggregation script.
---

## Stage 1: Context Ingestion & Verification
- [ ] **Global Context Review:** Exhaustively review the following core project files to ensure full alignment with project standards:
    - [ ] `README.md` (root)
    - [ ] `PLANNING.md`
    - [ ] `TASKS.md`
	- [ ] `docs/overview.md`
    - [ ] `docs/architecture.md`
    - [ ] The complete rule suite in the `.windsurf/rules/` directory.
- [ ] **Phase-Specific Context Review:** Exhaustively review the following files to understand the present phase and workflow context:
    - [ ] `phases/01_LegacyDB/README.md`
    - [ ] `phases/01_LegacyDB/PLANNING_PHASE1.md`
- [ ] **Task-Specific Context Review:** Exhaustively review the following files to understand the specific requirements of task `P1.W3.T3.1`:
    - [ ] `phases/01_LegacyDB/PLANNING_PHASE1.md#Testing-Strategy-for-Aggregation-Script`
    - [ ] The target script `phases/01_LegacyDB/src/04_run_comparison.py`
    - [ ] Any existing logs in `phases/01_LegacyDB/src/`.

## Stage 2: Preparation
- [ ] Confirm prerequisite execution task P1.W3.T2.1 succeeded (outputs exist).
- [ ] Create pytest fixture to supply sample metric CSVs or use real pipeline
      outputs for integration-style assertions.
- [ ] Define acceptance criteria: CSV row counts, Markdown headings, correct
      numeric aggregates.

## Stage 3: Execution
- [ ] Create `phases/01_LegacyDB/tests/p1_w3/test_04_run_comparison.py` implementing:
      1. Happy-path end-to-end run via `subprocess` or function call.
      2. Edge-case test with missing metric file (expect graceful error).
      3. Assertion that output files are created and contain expected markers.
- [ ] Run `pytest phases/01_LegacyDB/tests/p1_w3/test_04_run_comparison.py` repeatedly until pass.

## Stage 4: Final Validation & Cleanup
- [ ] Confirm the test suite `phases/01_LegacyDB/tests/p1_w3/test_04_run_comparison.py` exists, all tests pass, and achieve 100% coverage (if feasible, otherwise document why not).
- [ ] Remove or git-ignore any large temp files generated by tests.
- [ ] Propose TASKS.md status change of P1.W3.T3.1 to `done`.
