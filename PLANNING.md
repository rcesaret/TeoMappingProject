# Digital TMP â€“ PLANNING.md

AI assistants **MUST** reference this file at the start of each coding or documentation session to stay aligned with the overall architecture, deliverables, and reproducibility vision.

This file provides strategic, narrative context for collaborators and AI assistants. Machineâ€‘enforceable conventions now live in `global_rules.md` (generic) and `.windsurfrules` (TMPâ€‘specific). If procedure or behaviour guidance in this file ever conflicts with those rule files, **the rule files prevail**.

---

## 1Â Â ProjectÂ Summary

This project modernizes and unifies the legacy datasets of the **Teotihuacan Mapping Project (TMP)**, one of the most comprehensive archaeological surveys in the Americas. The initiative converts fragmented analog and digital recordsâ€”including field notes, MSâ€¯Access databases, and handâ€‘digitized mapsâ€”into a fully reproducible PostgreSQL/PostGIS infrastructure for scholarly research, heritage management, and public dissemination.

The effort proceeds through five sequential, modular phases:

| Phase                          | Core Objective                                                                                              | Principal Outputs                                  |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------- | -------------------------------------------------- |
| **1â€¯Â DatabaseÂ Analysis**       | Audit four legacy TMP databases (DF8,Â DF9,Â DF10,Â REANS2) and migrate them into PostgreSQL                   | Migrated DBs, schemaâ€‘audit report                  |
| **2â€¯Â Databaseâ€¯Transformation** | Produce cleaned, denormalised wideâ€‘format datasets (`TMP_DF12`, `TMP_REANS_DF4`) with complete metadata     | Two analytical tables + YAML/Markdown dictionaries |
| **3â€¯Â GISâ€¯Digitization**        | Digitise archaeological, architectural, and environmental features from 1:2,000 raster base maps            | Validated vector layers + metadata                 |
| **4â€¯Â Georeferencing**          | Align digitised layers with global CRSs via highâ€‘precision control points and custom affine/NTv2 transforms | Aligned GIS layers, transformation logs            |
| **5â€¯Â GeospatialÂ Integration**  | Fuse tabular and spatial assets into a unified PostGIS database and publish derivatives                     | Unified geodatabase; archiveâ€‘ready exports         |


---

## 2Â Â ProjectÂ Architecture

The project is decomposed into three nested units:

* **Phases** â€“ macroâ€‘level milestones (see table above)
* **Workflows** â€“ cohesive processes within a phase
* **Tasks** â€“ atomic work items tracked in `TASKS.md`

### 2.1Â Â Phase Overview Table

| Phase                             | Description                                                | Key Outputs                                        |
| --------------------------------- | ---------------------------------------------------------- | -------------------------------------------------- |
| PhaseÂ 1Â â€“ DatabaseÂ Analysis       | Analyse legacy TMP databases; produce redesign proposal    | PostgreSQL DBs + schema audit report               |
| PhaseÂ 2Â â€“ DatabaseÂ Transformation | ETL into cleaned, denormalised datasets ready for analysis | `TMP_DF12`, `TMP_REANS_DF4` + metadata             |
| PhaseÂ 3Â â€“ GISÂ Digitization        | Manual digitisation of archaeological features             | Vector GIS layers (GeoJSON, Shapefile, GeoPackage) |
| PhaseÂ 4Â â€“ Georeferencing          | Apply controlâ€‘pointâ€‘driven CRS transformations             | Aligned GIS layers, transformation logs            |
| PhaseÂ 5Â â€“ GeospatialÂ Integration  | Merge tabular + spatial data in PostGIS                    | Unified geodatabase, public exports                |

### 2.2Â Â Modular Phase Breakdown

<details>
<summary>Phase 1Â â€“Â DatabaseÂ Analysis</summary>

* **WorkflowÂ 1.1**Â â€”Â Set up PostgreSQL versions of DF8,Â DF9,Â DF10,Â REANS2
* **WorkflowÂ 1.2**Â â€”Â Evaluate and compare schemas; draft redesign proposal
* **Output:**Â Schema audit report + redesign recommendations

</details>

<details>
<summary>Phase 2Â â€“Â DatabaseÂ Transformation</summary>

* **WorkflowÂ 2.1**Â â€”Â ETL and integration into wideâ€‘format dataframes
* **WorkflowÂ 2.2**Â â€”Â Variableâ€‘level cleaning, recoding, and feature engineering
* **WorkflowÂ 2.3**Â â€”Â Build metadata (data dictionaries, QA reports)
* **Output:**Â `TMP_DF12`, `TMP_REANS_DF4` + YAML/Markdown metadata

</details>

<details>
<summary>Phase 3Â â€“Â GISÂ Digitization</summary>

* **WorkflowÂ 3.1**Â â€”Â Construct highâ€‘resolution raster mosaics
* **WorkflowÂ 3.2**Â â€”Â Digitise archaeological and environmental features
* **WorkflowÂ 3.3**Â â€”Â Apply classification tags, validate topologies
* **WorkflowÂ 3.4**Â â€”Â Generate GIS layer metadata
* **Output:**Â Validated vector GIS files with ISOÂ 19115 metadata

</details>

<details>
<summary>Phase 4Â â€“Â Georeferencing</summary>

* **WorkflowÂ 4.1**Â â€”Â GCP calibration and transformationâ€‘model selection
* **WorkflowÂ 4.2**Â â€”Â Apply custom CRS transformations using PROJÂ +Â GDAL
* **Output:**Â Aligned GIS layers (UTMâ€¯14N/WGSâ€¯84), transformation logs

</details>

<details>
<summary>Phase 5Â â€“Â GeospatialÂ Integration</summary>

* **WorkflowÂ 5.1**Â â€”Â Load datasets into PostGIS
* **WorkflowÂ 5.2**Â â€”Â Perform spatial joins and crosswalk generation
* **WorkflowÂ 5.3**Â â€”Â Engineer spatial features; publish/export outputs
* **Output:**Â Unified PostGIS geodatabase; public archiveâ€‘ready files

</details>

---

## 3Â Â RepositoryÂ Structure

The repository follows a modular structure aligned with project phases and workflows. Key folders include:

```
<repoâ€‘root>/
â”œâ”€â”€ .windsurfrules
â”œâ”€â”€ PLANNING.md
â”œâ”€â”€ TASKS.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€â”€data
â”‚   â”œâ”€â”€â”€external
â”‚   â”œâ”€â”€â”€interim
â”‚   â”œâ”€â”€â”€processed
â”‚   â””â”€â”€â”€raw
â”œâ”€â”€â”€docs
â”‚   â””â”€â”€â”€drafts
â”œâ”€â”€â”€infrastructure
â”‚   â”œâ”€â”€â”€db
â”‚   â”‚   â””â”€â”€â”€legacy_db_sql_scripts
â”‚   â””â”€â”€â”€docker
â”œâ”€â”€â”€knowledge_base
â”œâ”€â”€â”€notes
â”œâ”€â”€â”€outputs
â”œâ”€â”€â”€phases
â”‚   â”œâ”€â”€â”€01_LegacyDB
â”‚   â”‚   â”œâ”€â”€â”€drafts
â”‚   â”‚   â”œâ”€â”€â”€notebooks
â”‚   â”‚   â”œâ”€â”€â”€outputs
â”‚   â”‚   â””â”€â”€â”€src
â”‚   â”œâ”€â”€â”€02_TransformDB
â”‚   â”‚   â”œâ”€â”€â”€drafts
â”‚   â”‚   â”œâ”€â”€â”€notebooks
â”‚   â”‚   â”œâ”€â”€â”€outputs
â”‚   â”‚   â””â”€â”€â”€src
â”‚   â”œâ”€â”€â”€03_DigitizeGIS
â”‚   â”‚   â”œâ”€â”€â”€drafts
â”‚   â”‚   â”œâ”€â”€â”€notebooks
â”‚   â”‚   â”œâ”€â”€â”€outputs
â”‚   â”‚   â””â”€â”€â”€src
â”‚   â”œâ”€â”€â”€04_Georef
â”‚   â”‚   â”œâ”€â”€â”€drafts
â”‚   â”‚   â”œâ”€â”€â”€notebooks
â”‚   â”‚   â”œâ”€â”€â”€outputs
â”‚   â”‚   â””â”€â”€â”€src
â”‚   â””â”€â”€â”€05_GeoDB
â”‚       â”œâ”€â”€â”€drafts
â”‚       â”œâ”€â”€â”€notebooks
â”‚       â”œâ”€â”€â”€outputs
â”‚       â””â”€â”€â”€src
â”œâ”€â”€â”€project_materials
â”œâ”€â”€â”€report
â”‚   â”œâ”€â”€â”€appendices
â”‚   â”œâ”€â”€â”€drafts
â”‚   â””â”€â”€â”€figures
â””â”€â”€â”€tests
```



### ğŸ“ Root
- `.env`, `.env.example` â€“ project-specific credentials
- `requirements.txt` â€“ primary dependency list (pip-style)
- `.gitignore` â€“ ignores unused databases, drafts, local metadata, system junk
- `.windsurfrules` â€“ Windsurf IDE config

### ğŸ“ `phases/`
Structured by project phase:
- `01_LegacyDB/`, `02_TransformDB/`, `03_DigitizeGIS/`, `04_Georef/`, `05_GeoDB/`
  - Each phase includes:
    - `src/` â€“ Python scripts
    - `notebooks/` â€“ QA or prototyping
    - `outputs/` â€“ final artifacts
    - `drafts/` â€“ working documents
    - `README.md` + `metadata.json` â€“ workflow description and schema

### ğŸ“ `data/`
- `raw/`, `interim/`, `processed/` â€“ DVC-friendly data lifecycle
- `external/` â€“ Dropbox-downloaded datasets (raster tiles etc.)
  - Ignored from Git via `.gitignore`

### ğŸ“ `infrastructure/`
- `db/legacy_db_sql_scripts/` â€“ Legacy database SQL exports
- `docker/` â€“ Reserved for late-stage containerization
- `cloud_downloads.md` â€“ Cloud import scripting guidance

### ğŸ“ `docs/` and `project_materials/`
- `architecture.md`, `overview.md`, `methods.md`, `data_sources.md`, `outputs_summary.md`, `references.md` â€“ human-readable project docs
- `CRS_Catalogue.csv` - All sanctioned spatial reference systemsâ€”including two custom *MillonÂ Space* CRSsâ€”are defined here. Extend via PR only.
- `tDAR/` â€“ archival formatting and metadata standards
- `TMP_Project_DS_Portfolio_OptimizStrategy/` â€“ strategy documents and summaries

### ğŸ“ `tests/`
- Unit and integration tests by phase, mirroring `phases/`

### âš ï¸ Files Without Version Control (See `.gitignore`)
- All `drafts/` folders
- `data/external/ms_raster_tiles`
- System/editor-specific folders (`.idea/`, `.vscode/`)
- `project_materials/` contains project materials not for the AI
- `knowledge_base/` contains knowledge files approved for the AI
- Local notebooks, cache folders

### Notes on Large Files

- **GitÂ LFS**Â manages large rasters and project imagery.
- **DVC** (optional) can track heavy data evolution beyond Gitâ€‘LFS capacity.

---

## 4Â Â TechÂ Stack, Tools, and Dependencies

### ğŸ Core Programming Stack
- **Language**: Python 3.11+
- **Notebooks**: Jupyter (used for QA and geospatial EDA in Phases 2â€“5)
- **Environments**: 
  - Use `conda` for dependency management, only using `pip` secondarily when a package is not available on `conda`
  - Use `.env` for storing credentials (being sure to add placeholders to `.env.example` as well)
- **Databases:**Â PostgreSQLÂ 17 with PostGIS
- **GIS Desktop:**Â QGISÂ 3.40+
  
  
###Â 4.1Â Â Core Programming Stack

- **Language:**Â PythonÂ 3.11+
  (Version policy & upgrade path are enforced in `.windsurfrules`Â Â§2.)
- **Notebooks:**Â Jupyter (used for QA & EDA in PhasesÂ 2â€“5)
- **Database:**Â PostgreSQLÂ 15+ with PostGIS
- **GIS Desktop:**Â QGISÂ 3.40+

###Â 4.2Â Â Key Python Libraries

- DataÂ / ETLÂ â€”Â `pandas`, `numpy`, `sqlalchemy`, `pydantic`, `great_expectations`
- GeospatialÂ â€”Â `gdal`, `ogr`, `rasterio`, `fiona`, `geopandas`, `shapely`, `pyproj`, `whitebox`
- GeoreferencingÂ â€”Â `ntv2`, `affine`, `pyprojâ€‘transformer`
- TestingÂ â€”Â `pytest`, `pytestâ€‘cov`, `pandas.testing`, `geopandas.testing`, and `geopandas.testing`, and `great_expectations` (where applicable)

###Â 4.3Â Â Metadata & Documentation

- **Markdown** for design notes; **YAML** sideâ€‘cars for dataset metadata.
- **tDAR exports:** metadata mapped to tDAR schema.

###Â 4.4Â Â Continuous Integration and Quality Gates

All automated enforcement (coverage floor, cyclomatic complexity, preâ€‘commit hooks, schemaâ€‘diff, etc.) is defined in `.windsurfrules`Â Â§Â§7â€“9. CI runs on GitHubÂ Actions.

---

##Â 4.6Â Â Operational Standards

### ğŸ³ Dockerization & Deployment
- Dockerization will occur **only at the end of the project**.
- No Docker/k8s work should begin unless explicitly requested and scheduled by the user.
- Final deployment will ship PostgreSQLâ€¯+â€¯PostGIS and a readâ€‘only API in separate containers for portability.

### ğŸŒ GIS Metadata and File Standards
- All digitized geospatial outputs must include metadata compliant with **ISO 19115**.
- Vector files must be validated with topologies and CRS metadata embedded.
- Produce and use `.geojson`, `.shp`, and `.gpkg` formats for maximum compatibility.
- Store and maintain GCPs, transformation matrices, and raster alignment logs alongside spatial products.

### â¬‡ï¸ External Data Downloads
- Large datasets stored externally will be downloaded via **Python scripts** using **Dropbox direct links**.
- All download scripts must:
  - Validate checksum (e.g., SHA-256) after download
  - Write a `.download.log` file with timestamp, source, file name
  - Default target directory: `data/external/`

### ğŸ“¦ tDAR-Compatible Output Requirements
When preparing export-ready files for tDAR (The Digital Archaeological Record):
- All metadata must adhere to tDARâ€™s structured templates for:
  - **Datasets** (CSV, XLSX, MDB)
  - **GIS Files** (SHP, PRJ, DBF, XML)
  - **Documents** (PDF, DOCX)
  - **Ontologies** (.owl format or tab-indented list syntax)
- Required metadata includes:
  - Title, year, creators, institutions, roles
  - Spatial and temporal coverage
  - Site terms, cultural periods, material types
  - Investigation type, archive source, and licensing/copyright info
- Use structured YAML or Markdown `metadata.yaml` for each output set to support tDAR ingest scripts.

---

## 5Â Â Authorial WritingÂ Style Guide

> **Reference location for AI:** all sample writings are indexed under `knowledge_base/user_style/`. Always consult them before generating or rewriting narrative text.

###Â 6.1Â Â Stylistic Fundamentals

| Dimension                 | Guideline                                                                                                                         |
| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| **Tone**                  | Formal, analytic, and objective; avoid conversational idioms.                                                                     |
| **Sentence construction** | Prefer long, multiâ€‘clause sentences that layer evidence and qualification (average 25â€“35 words), but ensure syntactic clarity.    |
| **Hedging & precision**   | Use qualifiers such as *suggests, indicates, likely, appears* to convey uncertainty.                                              |
| **Citations**             | Parenthetical Chicago Authorâ€‘Date style: (AuthorÂ Year:Â page). Multiple sources separated by semicolons.                           |
| **Terminology**           | Employ disciplineâ€‘specific vocabulary from archaeology, historiography, economic history, and data science. Verify term accuracy. |
| **Transitions**           | Use explicit linking phrases to signal logical progression (*â€œBuilding upon this premise â€¦â€*, *â€œBy contrast â€¦â€*).                 |
| **Figures & tables**      | Refer to them inâ€‘text as â€œFigureÂ Xâ€ or â€œTableÂ Yâ€; store assets under `outputs/figures/`.                                          |

###Â 6.2Â Â Structural Conventions

1. **Canonical section order**Â â€”Â *IntroductionÂ â†’ MethodologyÂ â†’ AnalysisÂ â†’ DiscussionÂ â†’ Conclusion*.
2. **Bulletâ€‘toâ€‘prose transformation**Â â€”Â Convert lists into cohesive paragraphs while preserving logical hierarchy.
3. **Numbered signâ€‘posting**Â â€”Â For complex arguments, use ordinal adverbs (*First, Second, Third*) to guide the reader.
4. **Passive voice**Â â€”Â Acceptable where processes are foregrounded over actors; otherwise favour active constructions.
5. **Citation density**Â â€”Â Substantive claims require at least one citation; theoryâ€‘heavy passages may group citations at paragraph end.

###Â 6.3Â Â AI Writing Workflow

1. **Scope confirmation**Â â€”Â Ask the user for desired length and depth if ambiguous.
2. **Source mapping**Â â€”Â Break input into logical units; crossâ€‘reference sample corpus for stylistic anchors.
3. **Draft generation**Â â€”Â Apply the rules in Â§6.1â€“6.2; mirror paragraph cadence and citation frequency of samples.
4. **Selfâ€‘audit checklist**Â â€”Â Verify tone, sentence length, citation style, hedging language, and logical flow.
5. **User iteration**Â â€”Â Present draft, incorporate feedback, repeat audit.

*Failure to obtain clarification should trigger sensible defaults derived from this guide.*

---

## 6Â Â Further Reading

- `global_rules.md`Â â€”Â Crossâ€‘project conventions & human best practices.
- `.windsurfrules`Â â€”Â TMPâ€‘specific enforcement logic.
- `overview.md`Â â€”Â Project context, goals, background, project outline, architecture overview, general summaries.
- `architecture.md`Â â€”Â Detailed system design and dataâ€‘flow diagrams.
- `methods.md`Â â€”Â Analytical methods, modelling choices, and statistical procedures.

---

*End of PLANNING.md*
